{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Pixels One by One: Your First Autoregressive Image Generation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome, I am glad you are here!\n",
    "\n",
    "I'm [Tuna](https://tunahansalih.github.io). My world is pretty much all about image and video generation. It is what I focus on in my PhD and during my internships at places like Adobe (working on Firefly!) and Amazon AGI. For a while, I have been working with diffusion-based models, they are incredibly powerful. \n",
    "\n",
    "But the landscape of generative modeling is always growing, and I want to explore other types of generative models. Right now, I am diving into autoregressive models. I always find the best way to learn a topic is by trying to teach it to others. So, this blog post series is an attempt to teach myself the basics of autoregressive models, hoping you can learn something from it, too. I'll start with the basics and try to understand how these models work piece by piece.\n",
    "\n",
    "So, if you are curious about this side of the generative AI, let's get started and see how to generate images pixel by pixel!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, \"Autoregressive\". Let's break it down.\n",
    "\n",
    "You have already seen \"auto-regressive\" models in action even if you didn't call them that. At its heart, it basically means predicting the next outcome based on all the things that came before it.\n",
    "\n",
    "Think about how you type on your phone. When you write \"the weather is ...\", the keyboard will suggest completions based on the words you entered such as \"sunny\", \"rainy\", \"perfect for AI research\" (maybe not that last one). That is an auto-regressive model in action for language. \n",
    "\n",
    "Basically, predicting P(next word | all previous words) is an auto-regressive model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(mnist, batch_size=100, shuffle=True)\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "grid = torchvision.utils.make_grid(images, nrow=10)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 28\n",
    "N_PIXELS = IMG_SIZE * IMG_SIZE # 784\n",
    "CONTEXT_LENGTH = 5 # Let's say we look at the last 5 elements (can be start tokens or pixels)\n",
    "START_TOKEN_VALUE = -1.0 # A value distinct from our binarized pixels (0.0 or 1.0)\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "def binarize_transform(tensor_image):\n",
    "    return torch.where(tensor_image > 0.5, 1.0, 0.0).float()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    binarize_transform\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContextualPixelPredictor(nn.Module):\n",
    "    def __init__(self, input_size=CONTEXT_LENGTH, hidden_size=128, output_size=1):\n",
    "        super(ContextualPixelPredictor, self).__init__()\n",
    "        # Input is now a window of 'context_length' previous elements\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape [batch_size, CONTEXT_LENGTH]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # Output raw logits for the next pixel\n",
    "        return x\n",
    "\n",
    "model = ContextualPixelPredictor(input_size=CONTEXT_LENGTH, hidden_size=256, output_size=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_target_pairs(flat_images_batch, context_length, start_token_value):\n",
    "    \"\"\"\n",
    "    Prepares context windows and target pixels for a batch of flattened images.\n",
    "    Each image in the batch will be prepended with 'context_length -1' start tokens.\n",
    "    Then, for each actual pixel, its context is the 'context_length' elements\n",
    "    preceding it (which can be start tokens or actual pixels).\n",
    "\n",
    "    Args:\n",
    "        flat_images_batch (Tensor): Batch of flattened images [BATCH_SIZE, N_PIXELS]\n",
    "        context_length (int): The number of previous elements to use as context.\n",
    "        start_token_value (float): The value for the start token.\n",
    "\n",
    "    Returns:\n",
    "        contexts (Tensor): [BATCH_SIZE * N_PIXELS, context_length]\n",
    "        targets (Tensor): [BATCH_SIZE * N_PIXELS, 1]\n",
    "    \"\"\"\n",
    "    batch_size, n_pixels = flat_images_batch.shape\n",
    "    device = flat_images_batch.device\n",
    "\n",
    "    # Create padding of start tokens\n",
    "    # We need enough padding so that the first pixel has a full context window\n",
    "    # Example: context_length=3. Image: [p1, p2, p3]. Padded: [S, S, p1, p2, p3]\n",
    "    # Context for p1: [S, S, S_implicit_before_p1] -> no, this is not quite right.\n",
    "    # Context for p1: [S, S, S] (if p1 is the target)\n",
    "    # Let's define it as: to predict pixel `i`, use `padded_sequence[i : i + context_length]`\n",
    "    # and target is `padded_sequence[i + context_length]` if we shift the target.\n",
    "\n",
    "    # Simpler:\n",
    "    # Padded sequence: [S, S, ..., S (context_length times), p1, p2, ..., pN]\n",
    "    # To predict p1: context is [S, S, ..., S] (context_length times)\n",
    "    # To predict p2: context is [S, S, ..., S, p1]\n",
    "    # ...\n",
    "    # To predict p_k: context is [S, ..., p_{k-1}] (window of size context_length ending at p_{k-1})\n",
    "    \n",
    "    all_contexts = []\n",
    "    all_targets = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        single_flat_image = flat_images_batch[i] # Shape [N_PIXELS]\n",
    "        \n",
    "        # Prepend (context_length - 1) start tokens to the image\n",
    "        # If context_length = 1, no padding needed. If context_length=3, pad with 2 start tokens.\n",
    "        num_pads = context_length -1 # if context_length is 1, this is 0.\n",
    "        \n",
    "        # Corrected padding logic:\n",
    "        # To predict pixel p_j, the context is [x_{j-CONTEXT_LENGTH}, ..., x_{j-1}]\n",
    "        # where x can be a start token or a previous pixel.\n",
    "        # So, we effectively prepend CONTEXT_LENGTH start tokens to the sequence from which contexts are drawn\n",
    "        # and CONTEXT_LENGTH-1 start tokens before the first target.\n",
    "\n",
    "        # Create a sequence that includes start tokens for context building\n",
    "        # [S, S, ..., S (CONTEXT_LENGTH times), p0, p1, ..., pN-1]\n",
    "        padded_sequence = torch.cat([\n",
    "            torch.full((context_length,), start_token_value, device=device),\n",
    "            single_flat_image\n",
    "        ]) # Shape: [CONTEXT_LENGTH + N_PIXELS]\n",
    "\n",
    "        # Slide a window to create context-target pairs\n",
    "        for j in range(n_pixels): # For each actual pixel we want to predict\n",
    "            context = padded_sequence[j : j + context_length] # Window of size CONTEXT_LENGTH\n",
    "            target = single_flat_image[j].unsqueeze(0)      # The actual pixel p_j\n",
    "\n",
    "            all_contexts.append(context)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    contexts_tensor = torch.stack(all_contexts) # Shape [BATCH_SIZE * N_PIXELS, CONTEXT_LENGTH]\n",
    "    targets_tensor = torch.stack(all_targets)   # Shape [BATCH_SIZE * N_PIXELS, 1]\n",
    "    \n",
    "    return contexts_tensor, targets_tensor\n",
    "\n",
    "\n",
    "# Test the helper (optional, good for debugging)\n",
    "test_images = torch.rand(2, 5) # 2 images, 5 pixels each\n",
    "test_contexts, test_targets = create_context_target_pairs(test_images, context_length=3, start_token_value=-1.0)\n",
    "print(\"Test Contexts:\\n\", test_contexts)\n",
    "print(\"Test Contexts shape:\", test_contexts.shape) # Should be [2*5, 3] = [10, 3]\n",
    "print(\"Test Targets:\\n\", test_targets)\n",
    "print(\"Test Targets shape:\", test_targets.shape) # Should be [10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5 # Start with a few\n",
    "# CONTEXT_LENGTH, START_TOKEN_VALUE, N_PIXELS, IMG_SIZE, BATCH_SIZE assumed to be defined\n",
    "\n",
    "# --- Device Configuration ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# model and trainloader assumed to be defined and trainloader loaded\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Training on {device} with CONTEXT_LENGTH = {CONTEXT_LENGTH}\")\n",
    "\n",
    "# Outer loop for epochs with tqdm\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(trainloader)\n",
    "\n",
    "    # Inner loop for batches with tqdm\n",
    "    # Using position=0 for the outer loop and position=1 for the inner loop\n",
    "    # helps if you're running in a terminal where nested bars can get messy.\n",
    "    # For Jupyter, it usually handles nesting well automatically.\n",
    "    # `leave=True` for the epoch bar, `leave=False` for the batch bar (it disappears after completion)\n",
    "    batch_pbar = tqdm(enumerate(trainloader, 0), total=num_batches, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for i, data in batch_pbar:\n",
    "        images, _ = data\n",
    "        images = images.to(device)\n",
    "\n",
    "        flat_images = images.view(images.size(0), -1)\n",
    "\n",
    "        model_input, model_target = create_context_target_pairs(\n",
    "            flat_images, CONTEXT_LENGTH, START_TOKEN_VALUE\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(model_input)\n",
    "        loss = criterion(outputs, model_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update tqdm description with current loss\n",
    "        # We can calculate average loss so far in the epoch for a smoother display\n",
    "        if (i + 1) % 20 == 0: # Update description less frequently to avoid too much overhead\n",
    "             avg_loss_so_far = running_loss / (i+1) # Or running_loss / 20 if resetting running_loss\n",
    "             batch_pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"Avg Epoch Loss\": f\"{avg_loss_so_far:.4f}\"})\n",
    "\n",
    "\n",
    "    # After each epoch, you might want to print the average loss for that epoch\n",
    "    epoch_avg_loss = running_loss / num_batches\n",
    "    # The outer tqdm bar (for epochs) can also be updated\n",
    "    # tqdm.write(f\"Epoch [{epoch + 1}/{EPOCHS}] completed. Average Loss: {epoch_avg_loss:.4f}\")\n",
    "    # Or, if the epoch tqdm is the main one, its description can be updated,\n",
    "    # but usually, we just let it complete its iteration.\n",
    "    # If you want the final epoch loss to persist on the epoch bar:\n",
    "    # We can access the outer progress bar if we assign it:\n",
    "    # epoch_pbar = tqdm(range(EPOCHS), desc=\"Epochs\")\n",
    "    # for epoch in epoch_pbar:\n",
    "    #   ...\n",
    "    #   epoch_pbar.set_postfix({\"Avg Loss\": f\"{epoch_avg_loss:.4f}\"})\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_image_with_context(model, context_length, num_pixels=N_PIXELS, start_token_val=START_TOKEN_VALUE, device_str=\"cpu\"): # Added device_str\n",
    "    _device = torch.device(device_str) # Create device object from string\n",
    "    model.eval()\n",
    "    model.to(_device) # Ensure model is on the correct device for generation\n",
    "\n",
    "    current_context_list = [start_token_val] * context_length\n",
    "    generated_pixels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_pixels):\n",
    "            context_tensor = torch.tensor([current_context_list], dtype=torch.float).to(_device) # Move context to device\n",
    "\n",
    "            output_logits = model(context_tensor)\n",
    "            prob = torch.sigmoid(output_logits.squeeze())\n",
    "\n",
    "            next_pixel_dist = torch.distributions.Bernoulli(probs=prob)\n",
    "            next_pixel = next_pixel_dist.sample().item() # .item() brings it to CPU\n",
    "\n",
    "            generated_pixels.append(next_pixel)\n",
    "\n",
    "            current_context_list.pop(0)\n",
    "            current_context_list.append(next_pixel) # next_pixel is a Python float/int\n",
    "\n",
    "    img_array = np.array(generated_pixels).reshape(IMG_SIZE, IMG_SIZE)\n",
    "    return img_array\n",
    "\n",
    "# --- Device Configuration (defined earlier) ---\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Generate a few images\n",
    "num_images_to_generate = 5\n",
    "fig, axes = plt.subplots(1, num_images_to_generate, figsize=(10, 3))\n",
    "fig.suptitle(f\"Generated Digits (Context Length: {CONTEXT_LENGTH}, Device: {device})\") # Display device\n",
    "\n",
    "for i in range(num_images_to_generate):\n",
    "    # Pass the globally defined device string\n",
    "    generated_image_array = generate_image_with_context(model, CONTEXT_LENGTH, device_str=str(device))\n",
    "    axes[i].imshow(generated_image_array, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
